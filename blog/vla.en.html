<script type="application/json" data-front-matter>{"tags_zh": ["具身智能", "VLA", "多模态"], "tags_en": ["Embodied AI", "VLA", "Multimodal"]}</script>
<h1>On Embodied Intelligence (VLA)</h1>
<blockquote>
<p>Updated: 2025-10-03</p>
</blockquote>
<p>During my university years, AI has taken key leaps. From GPT-3 to widespread deployment of large models, I have witnessed the evolution from understanding to reasoning. Yet robots still lack human-like multimodal perception and physical interaction. True embodied intelligence integrates vision with human instructions to produce precise, generalizable actions (Vision-Language-Action).</p>
<h2>My view of VLA</h2>
<ul>
<li>Vision: stable perception and localization (Detection / Segmentation / Pose)</li>
<li>Language: goal-driven, interpretable task decomposition (Instruction → Plan → Act)</li>
<li>Action: generalizable low-level control and high-level policy (Imitation / RL / Model-Based)</li>
</ul>
<h2>Research directions</h2>
<ol>
<li>Multimodal alignment: unified representations of vision-language-action</li>
<li>Data collection: more realistic interactive data (teleoperation + synthetic)</li>
<li>Policy transfer: robust generalization from simulation to reality</li>
</ol>
<p>I aim to build robots’ “eyes and brain” to truly perceive and understand the physical world and interact effectively.</p>
<hr />
<p>If you’re interested in embodied intelligence, feel free to reach out: <code>chenxp68@mail2.sysu.edu.cn</code></p>