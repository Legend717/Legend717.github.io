---
tags_en: [Embodied AI, VLA, Multimodal]
tags_zh: [具身智能, VLA, 多模态]
---

# On Embodied Intelligence (VLA)

> Updated: 2025-10-03

During my university years, AI has taken key leaps. From GPT-3 to widespread deployment of large models, I have witnessed the evolution from understanding to reasoning. Yet robots still lack human-like multimodal perception and physical interaction. True embodied intelligence integrates vision with human instructions to produce precise, generalizable actions (Vision-Language-Action).

## My view of VLA
- Vision: stable perception and localization (Detection / Segmentation / Pose)
- Language: goal-driven, interpretable task decomposition (Instruction → Plan → Act)
- Action: generalizable low-level control and high-level policy (Imitation / RL / Model-Based)

## Research directions
1. Multimodal alignment: unified representations of vision-language-action
2. Data collection: more realistic interactive data (teleoperation + synthetic)
3. Policy transfer: robust generalization from simulation to reality

I aim to build robots’ “eyes and brain” to truly perceive and understand the physical world and interact effectively.

---

If you’re interested in embodied intelligence, feel free to reach out: `chenxp68@mail2.sysu.edu.cn`